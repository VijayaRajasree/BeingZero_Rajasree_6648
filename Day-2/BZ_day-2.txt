1. What is caching? Need?
A) Caching is a technique used to store copies of data in a temporary storage location, known as a cache.The primary goal of caching is to reduce latency, improve performance, and decrease the load on primary data sources .
Need of Cache:
->Performance Improvement.
->Reduce Load on Primary Storage.
->Cost Efficiency.
->Enhanced User Experience.

2. In-Memory Cache: Redis and Memcached Introduction?
A) In-Memory Cache stores data in the system's main memory (RAM) instead of on disk, allowing for extremely fast data access.
    i) Redis:
         ->Redis (Remote Dictionary Server) is an open-source, in-memory data structure store.
         ->It supports various data structures like strings, hashes, lists, sets, and sorted sets.
         ->Redis provides persistence options, replication, and high availability.
     ii) Memcached:
           ->Memcached is a high-performance, distributed memory caching system.
           ->It is simpler than Redis and primarily used for caching small chunks of data.
           ->Memcached does not support persistence or advanced data structures.

3. Cache Memory in Computer Organization?
A) Cache Memory is a small-sized, high-speed memory located close to the CPU in a computer system. It stores copies of frequently accessed data from the main memory to reduce the time taken for data retrieval.
->Levels of Cache Memory:
       L1 Cache: Fastest and smallest, located inside the CPU.
       L2 Cache: Larger than L1 but slower, often located on the CPU chip.
       L3 Cache: Larger and slower than L2, shared among multiple CPU cores.

4. Different Cache Replacement Strategies?
A) When a cache is full, a replacement strategy determines which item to evict to make space for new data. Common strategies include:
       ->Least Recently Used (LRU): Evicts the least recently accessed item.
       ->Least Frequently Used (LFU): Evicts the least frequently accessed item.
       ->Most Recently Used (MRU): Evicts the most recently accessed item.
       ->First In, First Out (FIFO): Evicts the oldest item in the cache.
       ->Random Replacement (RR): Randomly selects an item to evict.

5. Designing LRU Cache Using Double Linked List and Hashing
A) LRU Cache Design:
      Data Structures:
	Doubly Linked List: Maintains the order of items based on their usage. The most recently used item is at the front, and the 	least recently used item is at the end.
      Hash Map: Stores keys and their corresponding nodes in the linked list for O(1) access.
      Operations:
         ->Get(key):
	If the key exists, move the corresponding node to the front of the list (most recently used) and return the value.
	If the key does not exist, return -1.
         ->Put(key, value):
	If the key exists, update the value and move the node to the front.
	If the key does not exist, add a new node to the front. If the cache is full, evict the node at the end.

6. UML Diagram for LRU Cache Design
A)                          +-----------------------+
                              |     LRUCache           |
                              +-----------------------+
                              | - capacity: int         |
                              | - cache: HashMap  |
                              | - head: Node          |
                              | - tail: Node             |
                             +-----------------------+
                              | + LRUCache(int)    |
                              | + get(key): int        |
                              | + put(key, value)    |
                              +-----------------------+

                                 +--------------------+
                                  |      Node             |
                                 +--------------------+
                                  | - key: int            |
                                  | - value: int         |
                                  | - prev: Node      |
                                  | - next: Node      |
                                 +--------------------+